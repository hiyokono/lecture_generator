import os
import re
import json
import stanza
import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv
import anthropic

load_dotenv()  # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€

class WordListGenerator:
    def __init__(self):
        self.nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma')
        self.client = anthropic.Anthropic(
            api_key=os.environ.get("ANTHROPIC_API_KEY"),  # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰API keyã‚’å–å¾—ã€‚os.getenvã§ã¯ãªãos.environ.getã‚’ä½¿ã† ğŸ”‘
        )

    def get_target_files(self, book_dir):
        target_files = []
        for root, dirs, files in os.walk(book_dir):
            for file in files:
                if file.endswith(".md") and os.path.basename(root).startswith("week") and "example" not in root:
                    target_files.append(os.path.join(root, file))
        return target_files

    def analyze_text(self, text):
        doc = self.nlp(text)
        lemma_dict = {}
        for sentence in doc.sentences:
            for word in sentence.words:
                if word.upos not in ["PUNCT", "NUM", "PROPN"] and word.text.isalpha():
                    lemma = (word.lemma.lower(), word.upos)
                    lemma_dict[lemma] = lemma_dict.get(lemma, 0) + 1
        return lemma_dict

    def filter_easy_words(self, lemma_dict):
        df = pd.read_csv("data/CEFR-J.csv")
        easy_words = set(df[df['CEFR'].isin(['A1', 'A2', 'B1'])]['headword'])
        filtered_dict = {k: v for k, v in lemma_dict.items() if k[0] not in easy_words}
        return filtered_dict

    def generate_translations(self, text, lemma_list):
        prompt = f"ä»¥ä¸‹ã®è‹±æ–‡ã«å‡ºã¦ãã‚‹å˜èªã®ã†ã¡ã€ãƒªã‚¹ãƒˆã«ã‚ã‚‹å˜èªã®æ—¥æœ¬èªè¨³ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\n\nè‹±æ–‡:\n{text}\n\nå˜èªãƒªã‚¹ãƒˆ:\n{', '.join([lemma for lemma, _ in lemma_list])}\n\nå˜èªã®ä¸€èˆ¬çš„ãªæ„å‘³ã§ã¯ãªãã€è‹±æ–‡ã«å®Ÿéš›ã«å‡ºã¦ããŸæ„å‘³ã‚’æ—¥æœ¬èªã§æŒ™ã’ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªè¨³ã‚’è‹±èªã«è¨³ã—ãŸã‚‰ã€å…ƒã®å˜èªã«ãªã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªè¨³ã¯ã§ãã‚‹ã ã‘ç°¡æ½”ã«ã—ã¦ãã ã•ã„ã€‚è‹±æ–‡ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹å“è©ã¨æ—¥æœ¬èªè¨³ã®å“è©ãŒä¸€è‡´ã™ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚\n\nå‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã ã‘ã«ã—ã¦ãã ã•ã„ã€‚JSONä»¥å¤–ã®ã‚‚ã®ã¯è¿”ã•ãªã„ã§ãã ã•ã„ã€‚\n[{{\"lemma\": \"\", \"translation\": \"\"}}]"

        response = self.client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            temperature=0.5,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        }
                    ]
                }
            ]
        )

        json_text = response.content[0].text.strip()
        translations = json.loads(json_text)
        return translations

    def generate_word_list(self, input_file):
        with open(input_file, "r") as f:
            text = f.read()

        print(f"Analyzing {input_file}...")
        lemma_dict = self.analyze_text(text)
        print("Filtering easy words...")
        filtered_dict = self.filter_easy_words(lemma_dict)
        print("Generating translations...")
        translations = self.generate_translations(text, list(filtered_dict.keys()))

        output_file = os.path.join(os.path.dirname(input_file), f"words_{os.path.basename(input_file)}")
        # Create a dictionary to store lemma and count pairs
        lemma_to_count = {}
        for tr in translations:
            lemma = tr["lemma"]
            count = sum(value for key, value in filtered_dict.items() if key[0] == lemma)
            lemma_to_count[lemma] = (count, tr['translation'])

        # Sort the dictionary by count in descending order and convert to list of tuples
        sorted_lemma_counts = sorted(lemma_to_count.items(), key=lambda x: x[1][0], reverse=True)

        with open(output_file, "w") as f:
            for lemma, (count, translation) in sorted_lemma_counts:
                f.write(f"{lemma} ({count}å›) - {translation}\n")

        print(f"Word list generated: {output_file}")

    def run(self):
        book_dir = "book"
        target_files = self.get_target_files(book_dir)

        for file in tqdm(target_files, desc="Generating word lists"):
            self.generate_word_list(file)

if __name__ == "__main__":
    generator = WordListGenerator()
    generator.run()
